\cleardoublepage%
\phantomsection\addcontentsline{toc}{chapter}{Discussion}%
\chapter*{Discussion}

Three quotes were used to introduce this dissertation, all of which bear some relevance to the topic of neuroadaptive technology. As we have seen, neuroadaptive technology uses implicit input obtained from brain activity in order to adapt itself, e.g. to enable control or interaction, with implicit input referring to any actionable input to a system that was not intended as such by its source (cf. \citeNP{schmidt2000}). A quote from \citeApos{byrom1976dhammapada} admittedly liberal rendering of Chapter~I, Verse~1 of the Dhammapada, `With our thoughts, we make the world', may take on a new meaning in the context of implicitly brain-actuated environments. Although this is most probably not what the Buddha had in mind, an abstractly-formulated vision of neuroadaptive technology is to essentially allow our free thoughts to interact with the world unhindered, shaping it through continuous adaptations to the benefit of those persons whose thoughts have made it so. This may sound like a highly indulgent description of the matter, but at its core---unforced mental states adapt the external context to better support the human---it does embody the main benefits offered by even the most limited of today's laboratory demonstrations of neuroadaptive technology, let alone the applications envisioned for the future.

One necessary requirement for these benefits to be realised, is that our `thoughts' do indeed enter `the world' in some form, i.e. our brain activity must be externally monitored and decoded. This would add to the body of personal data that is already being gathered about virtually all users of modern technology. There exist multiple potential stakeholders to such data, including stakeholders like Edward Bernays, the late pioneer of using psychological insights to manipulate society to the benefit of paying customers, i.e., of propaganda. We should remain vigilant that additional insight into the `mental processes and social patterns of the masses' \cite{bernays1928propaganda}, which commercial neuroadaptive technology may enable to be gathered at scale and in real time, is not used against the public interest.

This is all the more pertinent in cases where the user may not be able to prevent information from being gathered in the first place. The concept of cognitive probing gives the computer some autonomy over the process of information gathering, and it has not yet been established that a user can counteract the automatic informative responses that are elicited by cognitive or affective probes. It is of such scenarios that the third quote serves to remind us, on page~\pageref{quote:locutus}.

These concerns may not appear to be immediately relevant today. It is true that many of the scenarios used to illustrate both potential benefits and potential adverse effects of neuroadaptive technology are based on speculation: for example, speculation concerning the general availability and adoption of brain monitoring hardware, speculation concerning the real-world accuracy of mental state classifiers, and speculation concerning our future living and working environments. Even this very dissertation introduced the topic with a speculative example of a neuroadaptive book. However, we are now in a position to make the same arguments without reference to thought experiments or science fiction. As also stated in the introduction, it is the main argument of this dissertation that neuroadaptive technology has now sufficiently progressed to warrant both widespread interest and widespread concern. A range of possible beneficial applications, as well as a number of important concerns regarding the use of neuroadaptive technology have been presented in Part~\ref{part:concepts} of this document. Subsequently, using tools and methodology developed in Part~\ref{part:tools}, two validations presented in Part~\ref{part:validations} demonstrate a number of key facts.

Specifically, Chapter~\ref{chapter:pbci} looked at existing passive BCI research and applications, and used a novel categorisation to identify a current trend \cite{krol2018interactivity}. It found that four different degrees of interactivity can be distinguished, starting with the theoretical zero-point which is required for pBCI to work: the ability in and of itself to assess mental states from recorded brain activity. Research is ongoing to produce and validate more accurate feature extraction and classification algorithms \cite<e.g.,>{lawhern2018eegnet,mousavi2019error,xu2020tangentriemannian} to improve and expand the computer's ability to decode our mental states, but a good number of proven methods already exist \cite{lotte2018classificationreview}. These methods can also be used in real time, allowing the computer to respond to the detected mental states. This makes the computer interactive, and enables it to adapt to the received implicit input---i.e, it enables neuroadaptive technology. The first interactive category includes open-loop systems, which adapt to the detection of pre-determined mental states in a pre-determined way. For example, the detection of a mental state reflecting the user's intention to interact with an object, as investigated by \citeA{protzak2013gazebased} and \citeA{shishkin2016wishmouse}, could be used to replace explicit control instructions such as button presses. The second interactive category deals with closed-loop neuroadaptive systems, in which the adaptations performed by the computer have an influence on the mental states that triggered the adaptation in the first place. In such closed-loop settings, neuroadaptive systems may for example detect workload levels and aim to bring them into balance \cite{kohlmorgen2007,afergan2014dynamicdifficulty}, or neuroadaptive learning environments can pace the learning experience to optimise student engagement \cite{yuksel2016bach,walter2017adaptivelearning}. The final category deals with automated adaptation, also known as intelligent adaptation \cite{fairclough2017intadapt}. Here, the computer employs a user model in which it stores the mental states it has decoded alongside the situational parameters in which they occurred, giving it a database from which to infer higher-order relations and causal effects. This concept has been used extensively by non-passive BCI systems aimed at explicit communication and control (see e.g. \citeNP{rezeika2018bcispellerreview} for a review), but a number of unique opportunities are created when it is employed by pBCI systems. Of particular interest is the fact that systems from this final category may have the ability to autonomously gather additional information needed to make more reliable inferences. The method that makes this possible, called cognitive probing, was discussed in more detail in Chapter~\ref{chapter:cp} and a demonstration was presented in Chapter~\ref{chapter:nat}.

Chapter~\ref{chapter:cp} explained in detail how neuroadaptive technology can use cognitive probing to obtain specific pieces of information. The newly-defined concept rests on the understanding that certain events automatically evoke responses from us. For example, it is virtually impossible not to read a word that is visually presented to us, and once we have read it, we have its semantic concept in mind. Also on a higher level, many cognitive and affective processes are beyond our conscious control: seeing or hearing something we like automatically brightens our mood, even if ever so slightly. These automatic responses are reflected in our brain activity. A system that has access to this brain activity, and the ability to interpret the appropriate mental states, could make use of this by presenting us with various stimuli and gauging our brain's automatic responses to them. One important reason why a system may want to do this is to update its user model. This model mentioned in the previous paragraph, where a user's mental state is recorded as a function of situational factors, may be incomplete, leaving the computer unable to infer information with the desired level of confidence. Cognitive probes can then be used to elicit the missing responses from the user, which are then stored in the model to fill in the gaps. As such, this method allows neuroadaptive systems to autonomously gather information from brain activity they deliberately induce in their users. A cognitive probe was defined as `a technological state change that is initiated or manipulated by the same technology that uses it to learn from the user's implicit, situational, cognitive or affective brain response to it' \cite{krol2020cognitiveprobing}. There is a steadily increasing number of examples in the literature that use various independently developed implementations of this method, which the provided definitions now allows to be recognised as sharing a fundamental similarity \cite<e.g.,>{dalseno2010,peck2013brainrecommender,iturrate2015teaching,kirchner2016multirobotcontrol,lorenz2016automaticneuroscientist,mladenovic2019activeinference}. Recognising the shared foundations of these previously separate approaches also allows a generic framework to be built in which both the benefits and the potential risks of the method can be described and analysed. The main benefit of implicit input itself is clear: it allows meaningful communication to take place without requiring any explicit instructions to be formulated. With cognitive probes specifically eliciting implicit input, the technology does not have to wait for the human to (implicitly) initiate the communication, but can autonomously gather the information it needs to best support the human. Furthermore, to the extent that it can target automatic responses, cognitive probing may be capable of obtaining information that could not be gotten any other way---not even by asking someone directly. As for the risks, importantly, cognitive probing requires the computer to have some autonomy in deciding what to present to the user, meaning it must also have its own logical reasoning to make these decisions. In other words, the computer can be seen to have its own \emph{agenda} \cite{fairclough2017intadapt} which may not necessarily align with the user's. With an adverse agenda, such a system may be able to extract responses from users against their will---to the extent that they are automatic---or induce undesirable mental states. Chapter~\ref{chapter:cp} therefore also discussed a number of technical aspects to consider, as well as ethical aspects concerning consent, privacy, agency, and responsibility. Care must be taken that autonomously operating systems with the ability to elicit and access implicit brain responses handle their influence and the obtained data with due care for their users.

The concepts and systems described in Chapters \ref{chapter:pbci} and \ref{chapter:cp} rely on real-time access to cognitive and affective states, decoded from a continuous recording of brain activity. The range of possible applications, as well as the severity of some of the ethical concerns surrounding these systems, thus depend on the exact types of cognitive and affective states that can be reliably detected. This is commonly determined through clever experimental designs aimed at uniquely eliciting specific mental states, but it is important to be able to verify these claims on the basis of the recorded data, and to make sure that the classifiers do in fact target the intended mental states. Chapters \ref{chapter:sereega} and \ref{chapter:visualisation}, therefore, introduced new tools to validate some of the analysis and classification methods that are in common use today.

Chapter~\ref{chapter:sereega} presented SEREEGA, a toolbox to simulate event-related EEG activity. It allows a signal comparable to the one normally recorded by real EEG electrodes to be produced from the ground up: a user of the toolbox can configure the exact parameters of the desired cortical processes, which will then be simulated to produce realistic scalp-level time series. The underlying parameters thus constitute a known ground truth describing the simulated processes. This is important, as such a ground truth is not available for real EEG recordings. Many EEG analysis techniques exist to attempt to uncover these `parameters' of real recordings, including the event-related potential technique to find true activation patterns in spite of noise and variability \cite{luck2014erp} and independent component analysis to find the cortical sources of the activations \cite{makeig1996}, and many more continue to be developed. To evaluate the efficacy and accuracy of these methods, however, a ground truth is needed to compare their results to. Therefore, researchers and developers often turn to simulations. Even though simulation has long been a popular method, no standardised toolbox existed for this purpose until the publication of SEREEGA \cite{krol2018sereega}. SEREEGA's functionality covers and extends the vast majority of past and present EEG simulation approaches, and the toolbox's architecture and open-source licence allows it to be readily extended with additional features. It makes EEG data simulation more accessible, standardised, and reproducible. 

SEREEGA-simulated data was used in Chapter~\ref{chapter:visualisation} to validate a classifier visualisation method. In data-driven approaches to BCI, machine learning is employed to select relevant features from the recorded data. These features are selected based on their utility to distinguish between the classes under investigation. In a way, achieving significant classification accuracy thus means that features can be identified that allow the classes to be reliably distinguished from each other. Importantly, however, in and of itself, a significant classification accuracy does not mean that the features selected by the classifier bear any relevance to the cortical processes that were intended to be targeted. If non-cortical activity is present in the data, which is almost always the case in raw EEG recordings, a classifier will make use of this activity whenever it aids the separability of the classes. For example, eye blinks and muscle activity often correlate to various task parameters. Such activity can thus be very informative, but may be liable to be influenced by irrelevant parameters as well: eye blinks, for example, may change as a function of environmental humidity. Therefore, it may be preferable to target cortical activity specifically. The method presented in Chapter~\ref{chapter:visualisation} visualises the areas that an EEG-based spatio-temporal classifier focuses on, allowing the different sources to be spatially separated, identified as cortical or non-cortical, and neurophysiologically interpreted. It does this by first transforming the classifier's filter weights into patterns \cite{haufe2014}, representing the projection of the areas isolated by the classifier onto the scalp (i.e., the forward model). These can then be decomposed into contributions from individual sources using a separately obtained unmixing matrix from blind source separation methods such as independent component analysis. This decomposition then assigns a so-called relevance weight to each source. Since the sources can be localised in 3D space, so can these relevance weights be assigned to a location. Using a 3D weighted kernel density plot, the weights can then be visualised in a virtual brain to identify which areas contributed to classification. This not only allows us to evaluate to what extent a classifier focused on cortical as opposed to non-cortical sources, it also provides insight into \emph{which} cortical areas, exactly, contribute to classification. With this method, we can thus even use classification methods to expand our understanding of the functional processes in the brain, introducing classifier-based neurophysiological analysis to neuroscientific research in general.

The tools introduced in Chapters \ref{chapter:sereega} and \ref{chapter:visualisation}, then, enabled us to validate the concepts introduced in Chapters \ref{chapter:pbci} and \ref{chapter:cp}---in particular, the notion of implicit control based on automatic brain responses elicited by cognitive probes.

The experiment described in Chapter~\ref{chapter:nat} demonstrated implicit control over a cursor. It consisted of two phases. In a first calibration phase, participants were shown a cursor moving randomly over the nodes of a $4\times4$ grid, with one of the corners indicated as the target. Participants were instructed to judge each individual cursor movement as either `appropriate' or `inappropriate' with respect to the larger goal of reaching the indicated target. A classifier was then trained on EEG data gathered in this first phase, separating `appropriate' from `inappropriate' movements with a mean estimated average accuracy of 72\% across participants. In a second online phase, this classifier was used to reinforce the cursor in real time. Initially, the cursor moved randomly: each cursor movement served as a cognitive probe, eliciting a response from the observing participant that served to categorise that movement as either appropriate or not. Based on that information, the model that reflected the participant's preferences with respect to the cursor movements was adjusted. Since the cursor's movements were based on this same model, the cursor came to behave increasingly in line with these preferences, even as the participants were unaware of causing any changes. As such, this online reinforcement based on implicit input led to significantly goal-oriented behaviour of the cursor, i.e., implicit cursor control. The classifier visualisation methods described in Chapter~\ref{chapter:visualisation} revealed that the classifier focused almost exclusively on the medial prefrontal cortex to extract the implicit input in question, supporting the interpretation that the underlying cortical processes are indeed automatic brain responses: they most likely relate to predictive coding, a fundamental process in which our brains continuously predict future (neuronal) events and compare those predictions to the actual final occurrences in order to optimise our behaviour. Such a fundamental process is unlikely to be under direct conscious influence, while at the same time, Chapter~\ref{chapter:nat} concluded, it can reflect higher-order cognition: in this case, it reflected fine-grained preferences with respect to cursor movements. Where traditional human-computer communication is restricted by the speed and accuracy with which humans can provide explicit instructions, this demonstration of effective cursor control through implicit interaction thus illustrated how neuroadaptive technology can bypass these limitations, effectively widening the communication bottleneck and decreasing the asymmetry present in current human-computer interaction paradigms.

In this demonstration of implicit cursor control, the classifier visualisation method was used to analyse which cortical processes significantly contributed to classification, and to support the conclusion that the used input was indeed implicit. However, it could not be determined with certainty that the targeted cortical activity was primarily informed by the participants' subjective preferences, as assumed, and not by more `neutral' cognitive processes such as those related to visual processing of the stimuli. For a deeper look at the processes underlying the implicit cursor control paradigm, Chapter~\ref{chapter:salval} therefore presented an adapted paradigm allowing two particular dimensions to be separated: visual salience and valence. The grid was larger, the corner nodes were removed, and a visually highlighted reference node was placed at the centre of the grid. As before, a cursor could move across the nodes of this grid. In two separate conditions, these visual (salience-related) aspects of the experimental paradigm were left constant, while the subjective (valence-related) interpretations of the cursor's movements were manipulated through the instructions: a movement towards the centre was `appropriate' in one condition, but `inappropriate' in the other. This allowed different classifiers to be calibrated focusing either on salience or on valence. The results of this analysis indicated that both salience and valence play a role during implicit cursor control as separate processes. Using the visualisation method from Chapter~\ref{chapter:visualisation}, we could reveal that the different classifiers could reliably isolate salience and valence from different cortical areas. Although this was certainly not the first experiment isolating valence-related activity from EEG, this experiment did demonstrate its presence in the implicit cursor control paradigm, and illustrate the utility of localising classifier weights in 3D source space for both BCI applications and neuroscientific research in general.

Furthermore, the fact that neuroadaptive technology can isolate and interpret brain activity indicative of subjective valence has important implications, concerning both the range of potential neuroadaptive applications, and the potential ethical issues that may arise from such applications. The results of these latter two chapters thus demonstrated the concepts described in Chapters \ref{chapter:pbci} and \ref{chapter:cp} as well as their potential benefits, but also validated the concerns addressed in those chapters. An important question thus remains: Going forward, as a field, how can we weigh what is possible against what is desirable?


\phantomsection\addcontentsline{toc}{section}{Value-Guided Research}%
\section*{Value-Guided Research}%

Already in 1980, Collingridge considered `one of the most pressing problems of our time: can we control our technology---can we get it to do what we want, and can we avoid its unwelcome consequences?' In this consideration, the avoidance of unwelcome consequences is the domain of governmental policies placing limitations and controls on the technology, determining what is allowed in what form. The problem in deciding this, however, is twofold. When a new technology is still in its infancy, its potential unwelcome consequences cannot be predicted with sufficient accuracy to justify any hard, legal limitations on its use. Given the unpredictable but inevitable and sweeping interactions that happen between technology and society, however, it is as good as certain that unwelcome consequences will arise at some point, but when these consequences finally surface, the technology has become so pervasive as to make limitations and controls difficult and expensive to implement. In Collingridge's words:

\begin{quote}
    This is the \emph{dilemma of control}. When change is easy, the need for it cannot be foreseen; when the need for change is apparent, change has become expensive, difficult and time consuming.
\end{quote}

This dilemma is now referred to as the Collingridge dilemma. \citeA{collingridge1980socialcontrol} illustrated it in the context of techno-societal issues such as the toxicity of lead in petrol, nuclear power plants, the Manhattan Project, and other forms of weapons research. We can see how it currently also applies to consumer neurotechnology in general, and neuroadaptive technology in particular. With commercial applications of neurotechnology now being marketed directly to consumers, but not yet having reached a point where other technologies, economic institutions, or social conventions rely on it, it would be minimally disruptive to limit or control the market at this point. At the same time, there is no pressing, justifiable reason for it: no harm is currently being done, clear potential benefits have been identified, and corporations big and small have explicitly shown interest in the technology. 

The precautionary principle is often suggested as a solution to this dilemma, essentially disallowing the technology until its safety has been proven. This principle is already employed by some regulatory bodies of the European Union, especially as regards the environment. A criticism of this approach, however, is that it stifles innovation. When dealing with the uncertainties of a technology's future influence on society, Collingridge instead proposes a method of decision making that recognises that the debate is essentially about values, but nonetheless focuses exclusively on facts. From his example of lead in petrol, a generally-accepted value $V_1$ may be formulated as \emph{If X eliminates a health hazard, X ought to be done}. Combined, then, with the fact $F_1$ that \emph{Reducing the amount of lead in petrol would eliminate a health hazard}, their conjunction $F_1 \land V_1$ leads to the conclusion $V_2$ that \emph{The amount of lead in petrol ought to be reduced}. The argument thus begins and ends with value statements, but hinges on $F_1$. Collingridge emphasises that, rather than debating values directly, a set of shared initial values allows the debate to focus on facts, allowing it to be argued on the basis of hard data. The environmental protection agency and the petro-fuel additive manufacturer never disagreed on $V_1$, instead arguing the evidence for and against $F_1$, using any number of additional facts.

This model could provide some structure to the ongoing discussion regarding potential ethical, legal, and societal implications of neuroadaptive technology. It suggests that we separate values from facts, and that a constructive step may be to find a set of relevant common values. 

A lot has been written about values relevant to neuroadaptive technology; in some cases, this is already in the form of clearly formulated values, in others, it is a broader analysis of how neuroadaptivity relates to a particular set of values. On many issues, however, which themselves are widely recognised as valid issues, no shared consensus has been reached. \citeA{fairclough2014confidential}, for example, argues that any physiological data must be treated with the same confidentiality as medical data, stating that `that is what they are'. \citeA{kellmeyer2018bigbraindata}, on the other hand, points out that there is no generally accepted definition of biomedical data that would presently allow us to classify brain data as such: as an analogy, he points out that medically-relevant information has been inferred even from supposedly non-medical sources of data such as text messages and a smart phone's location data. If brain activity is medical data because medical conclusions can potentially be drawn from it, should text messages be medical data too? This discussion, however, in part mixes values and facts. Both authors implicitly agree that \emph{personal data should be protected}, and that \emph{medical data should have higher standards of protection than personal data}. They furthermore agree with the fact that \emph{neurotechnology processes brain data} and that \emph{brain data is personal data}; they disagree, however, whether \emph{brain activity is medical data}. The Collingridge perspective thus focuses the debate on this latter fact, and suggests that new research should be conducted to support or contest this issue in particular, rather than continuing to discuss which values ought to apply.

Similarly, proponents of cognitive liberty argue that no individual may be compelled against their will to use neurotechnology, nor that they should be prohibited from doing so, as long as no harm befalls others \cite{boire2000cognitiveliberty}. They propose this as a necessary extension to the freedom of thought, stating that `the right and freedom to control one's own consciousness and electrochemical thought process is the necessary substrate for just about every other freedom' \cite{sententia2004cognitiveliberty}. Neuroadaptive technology, especially when using cognitive probing as described in Chapter~\ref{chapter:cp} \cite{krol2020cognitiveprobing}, may in fact interfere with such freedom in ways that are not clearly covered by the current concept of cognitive liberty, which assumes that the human controls the technology. If someone voluntarily chooses to use neuroadaptive technology, but this technology then uses implicit input to influence their mental states without this person's awareness---potentially even against their interests---to what extent, exactly, is this person free to choose their use of the technology? To what extent are they in control of the outcome when the outcome is produced from their implicit input? And by extension, who would bear responsibility for such an outcome? For example, the implicit cursor control experiment was designed to guide the cursor towards the intended target. Using the exact same input, however, it could also be inferred where the user did \emph{not} want the cursor to go---and the cursor could be steered there instead. The same data can be used both to support the user, and to generate the exact opposite outcome.
% while the reader of the neuroadaptive book, encountered in the introduction to this dissertation, may wish to read a happy story, the book itself can use the exact same input to instead generate the opposite of what the reader desires.
And even when the outcome is seemingly favourable, `although [humans] may be certain about what they wanted, they may be insecure about whether they did or did not ``do'' it, and even wrong about what they think they wanted' \cite{haselager2013agency}. 

This points to a deeper issue that may boil down to the definition of `control'. What does it mean, exactly, for someone to be in control of the neurotechnology they use? Fields such as control theory, also applied to human-computer interaction, consider this question to be exclusively confined to the realm of facts; for example, they assign `control' a fixed definition based solely on the mechanical or logical functioning of the technology \cite<e.g.,>{ieeedictionary}. However, neurotechnology allows humanity and technology to become increasingly interwoven, making a reliance only on the technological aspects seem one-sided. As we have seen, e.g. in Chapters \ref{chapter:pbci} and \ref{chapter:cp}, and in \citeauthor{haselager2013agency}'s comment concluding the previous paragraph, in a neuroadaptive interplay between user and technology it may no longer be as clear who does what, or who controls whom. In light of considerations to separate facts and values, it may therefore actually be worthwhile to consider control, as in `being in control', to be a value instead---a value that is yet to be defined and agreed upon. As a value statement, as opposed to a fact, a notion of `control' can give due consideration to ethical issues surrounding the use of technology. For example, it could allow a reader to be formally considered to implicitly control a neuroadaptive book if and only if the actions performed by the book are consonant with the reader's immediate wishes. Such a value statement can then be combined with facts pertaining to various neuroadaptive control mechanisms to ascertain whether a particular application does or does not enable `control', and by extension, whether the human is a `user' or not. This is of crucial importance to interpret statements, such as those referring to cognitive liberty, with respect to the freedom to \emph{use} neurotechnology and to \emph{control} one's consciousness. 

This additional perspective on Collingridge's original method thus forces us not to simply list known values and facts, but also to consider more closely whether some facts perhaps ought to be seen as values, and vice versa.

The explicit consideration of societal values also widens the perspective beyond issues concerning the immediate context of the technology and its individual user. The adoption of new technology by a society is unlikely to progress uniformly, and may affect different people in different ways. BCI research oriented at explicit communication and control applications has found that roughly 20\% of participants are not able to reliably generate a detectable control signal, in some cases simply because of the anatomical structure of their brains, which cannot be changed \cite{allison2010illiteracy}. It is unknown to what extent this may affect the various passive measures of cognition which form the basis of neuroadaptive technology, but it should be assumed that, for any given neurotechnology, a part of the population may simply not be able to use it. Conversely, when states can accurately be detected, there may be a part of the population for whom these states more often constitute invalid or counter-productive information than for others---people suffering from post-traumatic stress or obsessive-compulsive disorders, for example, may not be able to benefit from closed-loop neuroadaptive systems in the same way as others. Such considerations apply to all new technologies, of course, but their applicability to neuroadaptive technology may not be as predictable as for other technologies, which generally rely on explicit interaction.

It might thus be of interest to organise a series of workshops where a list of values may be gathered and discussed: a documentation of values that are relevant to neuroadaptive technology, the extent to which they are currently contested, and what facts would be required to either adhere to or conflict with them. Such a document could be more actionable than e.g. the list of four priorities that resulted from an earlier more general workshop \cite{yuste2017ethical}, as it would generate research questions with respect to facts that are either currently disputed, or currently unknown but bear direct relevance to the identified values. Accepting, for example, that the privacy of thought ought not to be violated, one can then design research to attempt to elucidate at what point any particular neuroadaptive application does indeed violate the privacy of thought \cite<e.g.,>{haselager2018reading}, and to investigate what may be done to protect it. Of particular impact, then, will be research producing new facts that demonstrate an application's incompatibility with society's fundamental values: such facts, combined with an authoritative list of said values, must lead to regulatory action, as per Collingridge's original model. Being guided by values, we may thus discover the vast benefits that neuroadaptive technology can offer society, by striving first and foremost to discover and demonstrate those forms of neuroadaptive technology that ought not to exist.


\phantomsection\addcontentsline{toc}{section}{In Conclusion}%
\section*{In Conclusion}%

The chapters of this dissertation presented conceptual, methodological, and experimental advances in the field of neuroadaptive technology. Part~\ref{part:concepts} provided conceptual frameworks to unify previous works and guide future research. Part~\ref{part:tools} presented new tools to help validate some of the technology's core methods. Part~\ref{part:validations}, finally, experimentally demonstrated a number of key possibilities. 

Honestly admitted, there is an inherent danger in mass-processing personal data, especially where it concerns uniquely sensitive, involuntarily produced brain data. Openly discussed, however, there is no need for the potential risks to outweigh the potential benefits. Properly guided, neuroadaptive technology may lead to environments where, through completely new notions of `interaction', our thoughts really do make the world.
