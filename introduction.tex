\cleardoublepage%
\phantomsection\addcontentsline{toc}{chapter}{Introduction}%
\chapter*{Introduction}

As evidenced by Figure~\ref{fig:donald} and a number of films including \emph{Eternal Sunshine of the Spotless Mind} (2004) and \emph{The Discovery} (2017), the idea of directly connecting our brains to machines has captured popular fascination. It is no longer limited to futuristic genres of science fiction, where such ideas had already taken root much earlier---at least since the 1950s. In \citeApos{anderson1957callmejoe} \emph{Call Me Joe}, for example, a wheelchair-bound man uses a `psionic' head-mounted device to project his neural activity across great distances, allowing him to live an all-too-real life in another being's body.\nocite{anderson1957callmejoe} \citeA{roelfsema2018mindreadingwriting} mention a novel on this topic going as far back as the early 1930s---the same decade in which, allegedly, even Nikola Tesla intended to investigate a `thought projector', allowing thoughts to be visualised, albeit not directly from the brain itself---the principle was supposedly for the optic nerve to be bidirectional, allowing brain activity representing imagined visuals to be read off the retina \cite{tesla1993fantastic}. Whereas this particular idea obviously did not come to fruition, we can look back on a number of actual milestones in the past near-century that illustrate remarkable progress in the field of \emph{neurotechnology}---an inclusive term essentially referring to any form of technology that monitors or manipulates activity in the central nervous system (CNS). Progress in this field is ongoing and accelerating, moving what was previously science fiction ever closer to reality. In the more recent past, research has moved outside of the traditional laboratories into more naturalistic settings \cite{makeig2009mobi}, direct-to-consumer neurotechnology has become widely available to the general public \cite{ienca2018brainleaks}, and internationally renowned companies have begun conducting and funding research in both medical and consumer applications, increasing public awareness even further \cite{musk2019,moses2019fbspeech}.

From science fiction, to popular entertainment, to reality: It is the main argument of this dissertation that \emph{neuroadaptive technology}---defined further below---has now sufficiently progressed to warrant both widespread interest and widespread concern. To that end, the works in this dissertation serve to examine and demonstrate the current reality with respect to neuroadaptive technology. This dissertation first describes some of the capabilities of neuroadaptive technology and how it can be used, both on a conceptual level and with respect to already-published work, highlighting the advantages of such technology as well as a number of potential risks. Tools are then introduced to support the analysis of an experimental demonstration of neuroadaptive technology. As this demonstration shows, it is now possible to implement control based on the brain activity of \emph{unwitting} participants, who remain unaware of having any influence even as their brain activity guides a virtual object. Furthermore, the final chapter demonstrates that neuroadaptive technology can access subjective value-related processes. These and other demonstrations illustrate how neuroadaptive technology can greatly benefit human-computer interaction by realising goal-oriented and supportive behaviours without requiring any effort from the user. At the same time, they illustrate how these applications require consideration of the user's rights with respect to, among other issues, informed consent, outcome responsibility, and privacy of thought.


\clearpage%
\phantomsection\addcontentsline{toc}{section}{A Brief History of Brain-Computer Interfacing}%
\section*{A Brief History of Brain-Computer Interfacing}%

One of the earliest milestones in the development of neurotechnology was achieved on July 6\textsuperscript{th}, 1924, when \citeA{berger1929humaneeg} first observed electrical activity in a human brain, using a technique that had previously been performed only on animals. A recording of such electrical brain activity over time is, following Berger's suggestion, called an \emph{electroencephalogram}, with the technique in general being referred to as \emph{electroencephalography}, and the abbreviation EEG being used for either of these two words. Already at that time it was known that this electrical brain activity was influenced by outside stimuli, such as a bright light shone into the eyes of the animal under investigation. Berger, however, was specifically interested in the influence of internal changes on the recorded EEG: he speculated that human EEG recordings might be used to diagnose medical conditions on the basis of pathological activity, and cautiously noted first indications---in his own son's EEG---that different intensities of mental activity led to visible changes in the recorded curves.

We now know that EEG does indeed reflect internal cognitive processes. Another significant development in that regard is the use of the \emph{event-related potential} (ERP) technique \cite{luck2014erp}. This was the first of a number of techniques that allowed researchers to systematically and accurately associate the brain's neuroelectric activity with specific events, and investigate this activity as a function of these events' physical or conceptual properties. Whereas the first such studies were probably performed in the late 1930s (\citeNP{davis1939erp} as cited by \citeNP{luck2014erp}), the utility of the technique was greatly improved by the later use of computers which could automatically gather multiple stimulus-response pairs and average the responses together, thus cancelling out brain activity that was not related to the event. This technique revealed clear cognitive components to the observed activity \cite<e.g.,>{walter1964cnv}.

The ERP technique thus allowed responses to specific events to be interpreted on the basis of a post hoc analysis of all gathered data. The first step towards interpreting event-related brain activity in \emph{real time} was taken in the 1970s, by \citeA{vidal1973direct}. In order to identify certain patterns of brain activity immediately following their occurrence, Vidal suggested `treating the experiment as a signal detection problem' \cite{vidal1977}: with continuous access to an ongoing EEG recording, a computer classified incoming data as belonging to one of four categories, based on previously learned (and continuously updated) decision strategies. Specifically, Vidal's apparatus flashed a bright chequerboard pattern in order to elicit activity in the visual cortex. Due to the retinotopic mapping of the visual cortex, this activity had a different spatial distribution depending on whether the human participant was looking at a point to the left, right, top, or bottom of the flashing pattern. The computer could decode this from the recorded brain activity in real time, allowing the participant to control the movement of an object on a computer screen in four directions.

With this project, Vidal coined the term \emph{brain-computer interface} (BCI; \citeNP{vidal1973direct}), now referring to any system that translates a measurement of CNS activity into artificial input to a computer, `thereby changing the ongoing interactions between the CNS and its external or internal environment' \cite{wolpaw2012newsun}. Where natural communication channels rely on muscular activity (e.g. to write, type, gesture, speak) or on hormonal changes (e.g. internal signalling, pheromones), a BCI thus establishes a different, part-artificial communication channel that bypasses these faculties, and provides a computer with real-time access to an interpretation of our mental states to the extent that they can be decoded from our brain activity.

Vidal speculated upon a wide range of potential future applications of BCI technology, including general neuroscientific research, computer-assisted learning tuned to optimal brain states, and, perhaps somewhat tongue-in-cheek, controlling spaceships. But it was in two of the fields he suggested that BCI first gained widespread attention: human-computer communication and neuroprosthetic control. In particular, BCI technology offered a unique potential to support paralysed or otherwise motor-impaired patients \cite{wolpaw2002}. It was these people, not students or astronauts, who stood to benefit the most from this technology. Therefore, the primary focus of BCI research has long been on developing a practical means for direct, brain-based communication and control. This has resulted in a number of different mental speller devices \cite<e.g.,>{farwell1988,treder2011gazeindepbci} and brain-actuated prostheses \cite<e.g.,>{mullerputz2008ssvepprosthesis,vansteensel2016alsimplant}, allowing patients to e.g. write letters \cite{birbaumer1999spelling}, control wheelchairs \cite{iturrate2009p300wheelchair}, browse the internet \cite{mugler2010p300browser}, paint \cite{muenssinger2010brainpainting}, or move artificial limbs \cite{wolpaw2008prosthetic} using only their brain activity. 

These and other applications have been improved throughout the past decades, in particular through improved reliability of the BCI methodology itself. Due to the non-stationarity of EEG activity, internal and environmental artefacts, and the general difficulty people can have in learning to modulate specific brain activity in and of itself, early applications sometimes required the user to be trained for many months before being able to meaningfully control a BCI system \cite{birbaumer2006commcontrol}. A major paradigm shift occurred when methods of machine learning were applied to BCI at the start of the current millennium \cite<e.g.,>{ramoser2000,blankertz2002singletrial,lotte2007classificationreview}. As opposed to users training to generate specific machine-mandated and machine-detectable patterns in their EEG, machine learning techniques allowed the training effort to be shifted to the computer: based on a large number of recorded samples, the machine could learn to extract more complex patterns from the user's EEG. These patterns could then also reflect less forced, less artificial, more natural aspects of human cognition such as imagined movement.

As a generic example, a BCI pipeline may consist of the following components. First, a \emph{training set} is recorded, containing brain activity that is indicative of at least two different mental states. This must not necessarily be done using EEG; magnetoencephalography, functional near-infrared spectroscopy, and functional magnetic resonance imaging are commonly used as well \cite<e.g.,>{mellinger2007megbci,solovey2012brainput,lorenz2016automaticneuroscientist}. These recordings usually represent a continuous stream of brain activity, from which the relevant segments must be extracted. A series of processing steps therefore reduce these segments to \emph{features}. The different mental states, now represented by different \emph{classes} of features, can then be described by the distributions of their corresponding features. A \emph{classifier} is then \emph{trained} or \emph{calibrated} on these features, learning their distributions. This classifier is then capable of \emph{classifying} newly incoming data as belonging to one of the previously-learned classes, based on where the newly extracted features of the incoming data fall within the previously-learned distributions. 

% Even with these improved machine learning techniques, only few patients actually use BCI devices. When even a small amount of muscle control remains, it is generally preferred to use this over BCI systems \cite{pasqualotto2015bciveye}, while patients for whom a BCI was thought to be the only viable option, i.e. completely locked-in patients, may in fact no longer have sufficient mental function to operate a BCI \cite{ramosmurguialday2011listoclis,birbaumer2012silence}. Despite significant successes, therefore, research targeting people with disabilities appears to be declining, with many publications now focusing on the possibilities BCI can offer to the healthy population \cite{eddy2019bcitrends}.

As these and other machine learning techniques allowed complex natural patterns of brain activity to be detected in real time, some of the ideas already speculated upon by Berger and Vidal were slowly rekindled: that this methodology could be used to detect and decode different naturally-occurring mental states, allowing computers to support us in our everyday tasks. These types of applications appeared to have been largely forgotten due to the BCI research community's focus on medical interventions, to the point that they were in fact excluded from a widely accepted definition of BCI at the time \cite{wolpaw2002}. At that same time, however, the field of human-computer interaction had a long history of exploring different naturalistic communication and interaction techniques \cite<e.g.,>{jacob2008realitybased}, and it was in this community that in 2008, different research groups presented the concept of using naturally-occurring mental states in human-computer interaction scenarios \cite{girouard2008fnirshci,cutrell2008passiveinput,zander2008bcinteraction}. In particular, Zander and colleagues presented a form of EEG-based `passive control', in which the addition of a BCI pipeline, which could detect and correct perceived errors without requiring additional voluntary actions from the users, led to a significant performance increase in an otherwise regular human-computer interaction scenario \cite{zander2008bcinteraction}. Zander's subsequently proposed formal categorisation of BCI applications expanded the prevailing definitions to include this category of \emph{passive BCI} systems, thus introducing the term \cite{zander2008enhancing,zander2011,krol2018interactivity}.

In passive BCI systems, the communication channel that is established carries input to the computer that was not intended as such by the human. For example, when a human operator becomes fatigued over time, or temporarily overburdened by increased task demands, this may lead to a detectable change in their brain activity, allowing a computer to automatically implement supportive measures. In such a case, the operator did not explicitly instruct the system to do so, nor did they voluntarily manipulate their brain activity; nonetheless, through this brain activity, the operator did provide input that resulted in these measures being taken. \emph{Implicit input} refers to input that was not intended as such by the human, but is nonetheless used as input by the computer \cite{schmidt2000,rotting2009implicit,zander2014implicit}.

Over time, the reintroduction of these ideas changed the field of BCI research, which had long stressed volitional communication and control. BCI researchers were initially divided on the question whether or not passive BCI systems should be considered examples of brain-computer interfacing at all \cite{nijboer2013asilomarsurvey}, and it was criticised that passive BCI's reliance on `intention' cannot be neuroscientifically operationalised \cite{wolpaw2012newsun}. However, as more applications and theories concerning passive BCI and implicit input were presented \cite<e.g.,>{rotting2009implicit,girouard2010fnirshci,zander2012context,kirchner2013brainreading}, the formal definition of BCI was updated in 2012 to embrace the concept \cite{wolpaw2012newsun}. Passive BCI applications were furthermore identified as one of the guiding principles for future BCI research \cite{brunner2015horizon2020}, and in the past years, the relative portion of research targeting people with disabilities appears to be declining, with an increasing number of publications now focusing on the opportunities passive BCI can offer to the healthy population \cite{eddy2019bcitrends}. 

At present, new machine learning methods continue to be developed and existing methods continue to be improved, providing increased reliability and opening up new applications for BCI technology \cite{lotte2018classificationreview}. For example, adaptive classifiers continuously update their parameters allowing them to track changing feature distributions \cite{shenoy2006adaptiveclassification,lotte2018classificationreview}, and transfer learning allows classifiers trained in one condition to be used in another, e.g. across sessions, across tasks, or across participants \cite{pan2010transferlearning,lotte2018classificationreview}. Furthermore, EEG hardware has become increasingly accessible to the general public \cite{ienca2018brainleaks}, tickling the public imagination, as e.g. evident from the various hackathons being organised in the field \cite{guger2019hackathons}. Whereas direct, explicit control continues to be a popular paradigm, human-computer interaction based on implicit input---i.e. \emph{implicit interaction}---is an avenue where BCI technology can have a truly unique impact. The most recent development in this field is the move towards neuroadaptive technology.


\phantomsection\addcontentsline{toc}{section}{Neuroadaptive Technology}%
\section*{Neuroadaptive Technology}%

What kinds of neurotechnology have authors of hard science fiction conceived of more recently, as possible future applications? Here is an excerpt from the Hugo-nominated novel \emph{Blindsight} \cite{watts2006blindsight}:

\begin{quote}
    Szpindel cleared his throat. ``Try this one.''

    The feed showed what she saw: a small black triangle on a white background. In the next instant it shattered into a dozen identical copies, and a dozen dozen. The proliferating brood rotated around the center screen, geometric primitives ballroom-dancing in precise formation, each sprouting smaller triangles from its tips, fractalizing, rotating, evolving into an infinite, intricate tilework...

    A sketchpad, I realized. An interactive eyewitness reconstruction, without the verbiage. Susan's own pattern-matching wetware reacted to what she saw---\emph{no, there were more of them; no, the orientation's wrong; yes, that's it, but bigger}---and Szpindel's machine picked those reactions right out of her head and amended the display in realtime. It was a big step up from that half-assed workaround called \emph{language}. The easily-impressed might have even called it mind-reading.
\end{quote}

The implication here\footnote{Confirmed through personal correspondence.} is that our brains (our `pattern-matching wetware') cannot help but react to the stimuli we perceive. When presented with something, our brains inevitably interpret it and produce an internal response, even if no explicit (e.g. verbal) response is required. The device described here essentially uses a passive BCI, detecting and interpreting these automatic responses. This implicit input is then used to adjust the display in a closed-loop fashion and to reconstruct, step by step, what Susan thinks she saw.

This is a prime example of neuroadaptive technology. Pending a more formal, peer-reviewed definition, neuroadaptive technology refers to any technology that uses implicit input obtained from brain activity in order to adapt itself, e.g. to enable control or interaction. The term `neuroadaptive technology' itself as representing this line of research was suggested by Scott Makeig and chosen by consensus at the Passive BCI Community Meeting in Delmenhorst, 2014, attended by experts from different fields working on similar or otherwise overlapping research, including physiological computing, cybernetics, brain-computer interfacing, computational neuroscience, neuroergonomics, and human-computer interaction. The term appears to have first been used, with largely this same meaning, in 2003 \cite{hettinger2003neuroadaptive}, even before passive BCI became a more prominent term. These days, passive BCI, referring to the interface itself, can more strictly be seen as a tool which can enable technology to be neuroadaptive.

To illustrate the concept in more detail as it may presently be understood, let us turn to a similar, more tangible example: imagine reading a neuroadaptive electronic book. The appearance is that of any other electronic book. As a human being, you are, to varying degrees, sympathetic to the characters in the story and sensitive to their various fates: when the fate of a beloved character appears to take a turn for the worse, you sympathise and become saddened. All this is a natural, involuntary reaction to the story's progress, and, in this example, is reflected in detectable changes in your brain activity. Our neuroadaptive book receives this emotional state as implicit input, and, being an electronic book, it also knows what page is currently being read and what happens on that page. Connecting your sudden change in emotional state with the context in which it appeared---our beloved character's setback---the book can infer your positive attitude towards this character. It can now re-write the upcoming pages on the fly to take advantage of this newly-gained information, and can continue to do so page after page, compiling a story uniquely catered to your implicitly communicated mindset as you keep reading.

Since the story's adaptations are happening on upcoming pages based on implicit input, the reader could potentially be wholly unaware of what is happening in the background, and yet, it is the input coming from that same reader that is somehow guiding the story. This means that, to the reader, the experience may be no different from that of any other book: the neuroadaptive experience requires no conscious voluntary actions, but simply happens based on activity that occurs naturally while reading. As such, however, the reader is at the mercy of the neuroadaptive logic, which may or may not be in line with the user's wishes: a reader who may want a happy story could instead be served their own personal worst ending.

Furthermore, an adaptive story, as it is committed to the book's pages, may reveal sensitive information when read back by someone else. A reader in whose individualised version evil prevailed, for example, may not want others to know their apparently preferred outcome. 

Finally, neuroadaptivity allows us to imagine an interesting scenario where the book does not have enough information to continue the plot line. When a decision is to be made between different paths but the preferences of the reader are unclear, the book could decide to postpone the decision and instead insert a chapter the primary purpose of which is not for the reader to be further entertained, but for the book to obtain further information regarding the reader's preferences. A number of situations can be presented simply to gauge the reader's responses, on the basis of which the necessary information to continue the main story can be inferred.

This example of a neuroadaptive book will stay with us throughout this dissertation, as it highlights a number of important aspects of neuroadaptive technology. It illustrates, for example, one of its main benefits: the implicit nature of the input means that the user does not have to exert any effort for this additional communication channel to be maintained. This makes it particularly useful in scenarios where high mental demand is placed on the operator, either to widen the human-computer communication bottleneck and make the interaction more symmetrical \cite{suchman1987hmcproblems,tufte1990}, to detect and alleviate the mental load using e.g. adaptive automation \cite{byrne1996adaptiveauto}, or to promote or sustain specific mental states.

For example, \citeA{kohlmorgen2007} has demonstrated how neuroadaptive technology can detect mental load during driving and automatically adjust secondary tasks to better suit the driver's current state, as one illustration of many possible uses in neuroergonomics and human-computer interaction \cite<e.g.,>{frey2016visualcomfort,mehta2013neuroergonomicsreview}. Vidal's suggestion to automatically detect mental states and tune adaptive learning systems accordingly has also been demonstrated to be feasible. \citeA{yuksel2016bach} presented a neuroadaptive learning system that automatically increased the difficulty level for students practising a musical piece whenever workload levels dropped below an individually-determined threshold. \citeA{walter2017adaptivelearning} demonstrated an arithmetic learning environment that both increased or decreased difficulty according to a measure of workload. In entertainment, \citeA{ewing2016tetris} introduced a game that uses implicit input in order to maximise the player's engagement; \citeA{krol2017meyendtris} proposed a similar concept using two separate dimensions of implicit input, thus additionally introducing an element of mental state balancing to the game. Entertainment overlaps with art in \citeauthor{ramchurn2019brainfilm}'s \citeyear{ramchurn2019brainfilm} proposal for a neuroadaptive film, switching between different narratives and sound designs based on a brain-based measure of a viewer's attention. Neuroadaptive technology has also been suggested to help with the contemplation of art itself \cite{krol2018museum}, or to infer personal preferences with respect to cultural heritage items in order to provide implicit tags or recommendations in real time \cite{karran2015culturalheritage}. We have also seen these developments in the context of neuroscientific research \cite{lorenz2017neuroadaptivebayesian}, where a neuroadaptive experimental design has been used to intelligently present different audiovisual stimuli in order to identify those stimuli that elicit the maximal response from the participant \cite{lorenz2016automaticneuroscientist}. Even tasks that are normally done using explicitly communicated commands, such as the control of a cursor or robotic arm, may be performed using neuroadaptive technology using implicit input elicited by movements of the cursor or robotic arm \cite{zander2014implicit,iturrate2015teaching}.

As such research illustrates and often emphasises, neuroadaptivity allows technology to support the user without placing any additional burden on them: the driver, for example, is automatically supported in real time without being required to undertake any explicit actions that would distract them from their main task, and visitors of the museum or movie spectators are given an individualised experience that they can focus on without explicitly needing to indicate their preferences at every turn.

The unique benefits of neuroadaptive technology, however, should be contrasted with its potential risks, which are of a similarly unique nature. The uniquely beneficial fact that neuroadaptive technology allows communication to take place without additional effort on the user's side, also means that it can happen outside of the user's awareness altogether. Furthermore, it may not be possible for users to limit or otherwise control the scope of this communication. Brain activity---the data at the heart of all neuroadaptive technology---is liable to contain more information than what is needed as input to a particular application. Additional information could be gathered accidentally, as e.g. incidental findings indicative of epilepsy \cite{acharya2013eegepilepsy} may be found in the recorded data, or, bad actors may deliberately attempt to obtain information outside of the bounds of necessity: imagine, for example, a neuroadaptive movie streaming service that also records your responses to advertisements. Furthermore, by design, the reciprocal nature of the system adaptations will be in a position to affect the mental states of the user. By and large, they will likely be designed to promote or sustain specific desirable mental states, such as a workload equilibrium or optimal learning engagement. A potential danger, however, lies in a mismatch between the system's target state and what states are acceptable or healthy for the user. Goal-oriented adaptive mechanisms can be said to constitute the system's own agenda \cite{fairclough2017intadapt}, and this agenda may or may not correspond to that of the user. These issues are compounded by the fact that implicit, not explicit, input is used: the user may have no control over the information that is being provided, and may be unaware of the use that is being made of the recorded data. 

Where issues related to the safety and privacy of neural data, informed consent, and transparency have been discussed recently in the context of brain-computer interfacing, this has primarily been done in the context of physiological or neural data in general and BCI-based explicit control in particular \cite<e.g.,>{fairclough2014confidential,ienca2016ethics,yuste2017ethical,kellmeyer2018bigbraindata}. Any discussion of neuroadaptive technology must deal with these issues and the unique additional concerns they raise in the context of implicit control.


\phantomsection\addcontentsline{toc}{section}{Current Issues Addressed in this Dissertation}%
\section*{Current Issues Addressed in this Dissertation}%

The highly interdisciplinary nature of the field of neuroadaptive technology has caused relevant research to span different communities, and its rapid development has left it without a shared terminology concerning a number of key developments. Part~\ref{part:concepts} therefore presents a perspective on previous research, highlighting different ways in which implicit input has been used and can be used to enable neuroadaptive technology. In particular, it focuses on one particularly powerful method that has been independently implemented a number of times, but deserves our collective attention.

Specifically, Chapter~\ref{chapter:pbci} first reviews existing passive BCI research and applications, and categorises them based on a dimension that has an important bearing on how the technology is used, or can potentially be used: interactivity, i.e., the technology's ability to respond to input---implicit input, in this case. The more interactive a technological system is, the more responsive it is, the more autonomous, and the better capable of adaptation. The theoretical zero point on this scale is the method of mental state assessment itself: a system that has the ability to decode a person's mental states, but does not use the obtained information for any interaction with that same person. Following this, the suggested categories of increasing levels of interactivity are open-loop adaptation, closed-loop adaptation, and finally automated adaptation, also known as intelligent adaptation \cite{fairclough2017intadapt}. An example of an open-loop adaptation is the correction of an error: when an operator commits or perceives an error, this can be decoded from their brain activity, and a system with direct access to the relevant implicit input could thus immediately correct the perceived mistake. In the case of closed-loop adaptation, the actions performed by the system on the basis of implicit input feed back to the user and influence the brain activity that triggered the adaptive action in the first place. This, for example, is implemented in adaptive automation systems where an implicit measure of workload is used to adjust automation levels in order to again influence the workload that is being monitored. In the last category, neuroadaptive systems use models to represent their user's implicit input along any number of dimensions, and base their responses on the information present in that model using goal-oriented control logic. This decouples the control logic from immediate mental states, and grants the system more autonomy to respond in different ways.

The interactivity perspective thus finally points towards systems that, given their autonomy, can also autonomously gather implicit input from their users. This method can make neuroadaptive technology particularly versatile. Chapter~\ref{chapter:cp}, therefore, considers this method in more detail. It reviews a number of works that have used a specific sequence of steps in their research: the autonomous elicitation of a brain response, the subsequent automated interpretation of this response, and finally, an instance of learning on the basis of this decoded interpretation. This sequence has been used by different researchers independently of each other, but, it is argued, gains particular relevance in the largely unexplored context of implicit interaction. In order to collectively discuss some of the technical and ethical issues that arise from this method, Chapter~\ref{chapter:cp} first proposes a definition that covers these previously disparate implementations, and suggests \emph{cognitive probing} as a label to refer to the method. 

Another issue concerns some of the fundamental difficulties of working with machine learning methods applied to brain data. Even with a clear conceptual understanding of what the technology is intended to do, care must be taken to validate the neural processes underlying the technology's actual functioning. For example, to the extent that cognitive probing is to be based on cortical processes taking place in the brain itself, it should be ruled out that the classifier makes use of non-cortical activity such as eye blinks or other muscular artefacts which do feature prominently in the EEG. This applies to all forms of neuroadaptive technology. Therefore, Part~\ref{part:tools} introduces two tools to help validate both the methods we use and the experiments we conduct in the field of neuroadaptive technology.

EEG is measured at the scalp, and although each electrode is at a spatially distinct location, it picks up electrical activity from all parts of the brain simultaneously---that is, all parts that generate activity at a sufficient scale for it to be measurable at the scalp. Because of this, EEG has a poor spatial resolution, and a significant amount of processing is required to interpret the recorded data. Unfortunately it is not possible to evaluate the analytical methods applied to EEG data against a ground truth, since no ground truth is available for EEG data. Instead, researchers turn to simulations of EEG data, where a ground truth can be manually constructed, allowing the results of newly developed methods to be compared to a known factual reference. Chapter~\ref{chapter:sereega} therefore presents SEREEGA (Simulating Event-Related EEG Activity), the first-of-its-kind free and open source toolbox designed to streamline and standardise the simulation of event-related EEG data. Using an architecture and feature set that covers and extends the vast majority of EEG simulation methods employed by researchers today, SEREEGA provides a scripting language and EEGLAB-based GUI \cite{delorme2004eeglab} to simulate realistic EEG data, thus providing a ground truth to evaluate and validate EEG analysis methods and pipelines.

Chapter~\ref{chapter:visualisation}, subsequently, uses SEREEGA to simulate data with a known ground truth in order to validate a source localisation method that visualises what areas of the brain a classifier focuses on. This is important information. Where many researchers rely on standardised experimental paradigms to elicit known cortical processes, this does not guarantee that these cortical processes are also targeted by the classifier. Similarly, post hoc analyses of recorded data to demonstrate that certain cortical processes were indeed elicited, for example through ERP analyses, provide no proof that these same processes contributed significantly to classification. In both cases, it is possible that the classifier instead focused primarily on other, more distinctive brain activity, including artefactual activity. Chapter~\ref{chapter:visualisation} therefore introduces a method that combines blind source separation with the filter weights produced by different types of classifiers, allowing these weights to be visualised in source space. The neurophysiologically uninterpretable filter weights are first transformed into interpretable patterns \cite{haufe2014}, and subsequently distributed onto the sources in a virtual brain such that each brain area's relative contribution to the classifier can be visualised. These so-called relevance weights can thus be used to analyse classifiers and inform statements as to which cortical processes, exactly, contributed to classification. Aside from that, this method also opens up new possibilities for classifiers to be used in neuroscientific research in general, opening up BCI methodology to a wider audience.
 
Part~\ref{part:validations}, finally, presents two validation studies based on the concepts from Part~\ref{part:concepts}, supported by the methods from Part~\ref{part:tools}. 

The first study, presented in Chapter~\ref{chapter:nat}, shows that it is possible to use cognitive probes to realise implicit cursor control. Participants observed a cursor on a screen that was initially moving randomly. Each movement served as a cognitive probe, eliciting a response from the observer that could be decoded in real time from their brain activity. This response contained information pertaining to their interpretation of each cursor movement, judging them as either appropriate or not with respect to reaching a desired target. Using this information, a user model could be generated that allowed the preferred movement directions to be inferred. Over time, the cursor was then steered towards the preferred target. Importantly, participants were unaware of having any influence over the cursor, even though it was their brain activity that enabled its goal-oriented behaviour. An analysis of the classifier supported this conclusion. As such, this demonstrated how even a quintessential case of explicit control---the movement of a cursor---can in fact be done implicitly, using cognitive probing as described in Chapter~\ref{chapter:cp}.

The final chapter, Chapter~\ref{chapter:salval}, dives deeper into the just-mentioned implicit cursor control paradigm in order to further investigate which cognitive processes contributed to what extent to classification. A new experiment was designed to dissociate cognitive processes related to visual perception (salience) on the one hand, and subjective value interpretations (valence) on the other. As we will see, both these processes are indeed present in the data, but separate classifiers can be constructed to focus primarily on one or the other. The visualisation method presented in Chapter~\ref{chapter:visualisation} allows us to localise the cognitive activity related to these separate processes in different cortical areas. Using appropriate classifier designs as confirmed by visualisation or other methods, it is thus possible to access brain activity related to subjective valence processing.

This conclusion emphasises that neuroadaptive technology can elicit and have access to human cognition in a goal-oriented fashion without these humans being aware of having any influence, or, indeed, of being influenced. As much as science fiction may have inspired speculation as to the possibilities of neurotechnology, and as much as speculation can remain useful to illustrate the possibilities---as in the example of the neuroadaptive book---previously fantastical speculations and possibilities have now largely left the realm of science fiction, and their legal, societal, and ethical implications must be given due consideration going forward.
