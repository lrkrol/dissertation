
\chapter{Passive Brain-Computer Interfaces: A Perspective on Increased Interactivity}
\chaptermark{Passive Brain-Computer Interfaces}%
\label{chapter:pbci}%


{\chaptermeta

\textbf{Krol, L. R.\textsuperscript{1}, Andreessen, L. M.\textsuperscript{1} \& Zander, T. O.\textsuperscript{1}}

{\small
\textsuperscript{1}Biological Psychology and Neuroergonomics, Technische Universität Berlin, Berlin, Germany

This is the postprint version of the manuscript published as:

Krol, L. R., Andreessen, L. M., \& Zander, T. O. (2018). Passive Brain-Computer Interfaces: A Perspective on Increased Interactivity. In C. S. Nam, A. Nijholt, \& F. Lotte (Eds.), \emph{Brain-Computer Interfaces Handbook: Technological and Theoretical Advances} (pp. 69-86). Boca Raton, FL, USA: CRC Press.\nocite{krol2018interactivity}
\par}}


\abstract%
Passive brain–computer interfaces (passive BCI; pBCI) have been introduced and formally defined almost a decade ago and have gained considerable attention since then. In this chapter, we clarify some points of confusion and provide a perspective on the past, present, and future of the field of passive BCI. This perspective concerns a key aspect with regard to which various pBCI-based systems differ from each other: interactivity. The more interactive a system is, the more responsive it is, the more autonomous, and the better capable of adaptation. Along these lines, we identify and describe four relevant categories of systems with varying levels of interactivity: mental state assessment, open-loop adaptation, closed-loop adaptation, and automated adaptation. We give examples of past and current research for each of these categories. The latter three are collectively introduced as neuroadaptive systems. This perspective and formal categorization helps to highlight human–computer interaction aspects that are relevant for the design of pBCI-based systems and points to possibilities for future research and development into passive BCI, implicit interaction, and neuroadaptive technology.


\clearpage


\fancypagestyle{pbci}{%
    \fancyhf{}
    \fancyhead[EC]{\textit{\leftmark}}
    \fancyhead[OC]{\textit{\rightmark}}
    \fancyfoot[C]{\thepage \\ \vspace{6.8mm} \colorbox{footerbg}{\parbox{\paperwidth-2\fboxsep}{\centering\parbox{0.75\paperwidth}{\centering\fontsize{9pt}{9pt}\selectfont\textcolor{footerfg}{This is the postprint version of published manuscript: Krol, L. R., Andreessen, L. M., \& Zander, T. O. (2018). Passive Brain-Computer Interfaces: A Perspective on Increased Interactivity. In C. S. Nam, A. Nijholt, \& F. Lotte (Eds.), \textit{Brain-Computer Interfaces Handbook: Technological and Theoretical Advances} (pp. 69-86). Boca Raton, FL, USA: CRC Press.}}}}}
    \fancyfootoffset[]{1.25in}}
\pagestyle{pbci}


\section{Introduction}
\label{pbci:introduction}

In 1977, Jacques J. Vidal demonstrated the real-time detection of ``brain events'', and offered an example of how these might be used to control a system. Instead of using offline, a posteriori methods to investigate neuroelectric brain activity, Vidal suggested ``treating the experiment as a signal detection problem'' \cite{vidal1977}: with continuous access to an electroencephalogram (EEG), a computer classified incoming EEG data as belonging to one of four categories, based on previously learned (and continuously updated) decision strategies. As such, the system was able to recognize specific brain activity in real time. 

Although Vidal speculated upon a wide range of potential future applications of this approach, it was in a clinical context that \emph{brain-computer interface} (BCI) methodology gained widespread attention. It was quickly realized that such direct BCIs could potentially provide a means for paralysed or otherwise motor-impaired people to communicate with the outside world without having to use their muscles \cite{wolpaw2002}. This resolve to help those people who stood to benefit most from this technology led to a strong focus on BCI for direct communication and control, and resulted, inter alia, in different mental speller devices \cite<e.g.,>{birbaumer1999spelling,brouwer2010tactilep300,farwell1988,furdea2009auditoryspeller,treder2011gazeindepbci} and brain-activated prostheses \cite<e.g.,>{mullerputz2008ssvepprosthesis,soekadar2016exoskeleton,wessberg2000primatemotorbci,wolpaw2008prosthetic}. For a long time, in fact, BCI research appeared to be inseparably connected to these applications. 

Improvements made over the decades have made the field more interdisciplinary and BCI methodology more reliable. Of particular significance was the introduction of machine learning techniques at the start of the current millennium \cite<e.g.,>{blankertz2002singletrial,lotte2007classificationreview,ramoser2000}. Previously, users of BCI systems were trained to generate specific machine-detectable features in their EEG, such as a specific frequency modulation at a specific electrode site \cite{birbaumer1999spelling}. These days, training has shifted to the machine, using supervised machine learning based on initial recordings \cite{blankertz2002singletrial}. As pattern recognition by the machine replaced operant conditioning of the user, the idea was rekindled that this methodology could also be applied to detect and investigate spontaneous, automatic brain activity, and that these detections could then be used as \emph{implicit input} to a system \cite{rotting2009implicit,zander2014implicit}. (We use ``spontaneous'' in its biological sense, meaning automatic, instinctive, involuntary, and inattentive; see below for a further discussion.)

Early examples of this can be found in suggestions to use error-related potentials to correct classification errors of traditional BCI systems \cite{blankertz2002,ferrez2005,ferrez2008simulated}. This was also suggested for response errors \cite{parra2003} and machine errors \cite{zander2008bcinteraction} during human-computer interaction (HCI) in general. But it was only in 2008, when BCI enjoyed increased popularity overall, that the approach gained widespread attention. The use of BCI methodology for implicit input to benefit ongoing HCI was explicitly proposed as a worthwhile research endeavor by two research groups at the same conference (CHI 2008, Florence, Italy), independently of each other \cite{cutrell2008passiveinput,zander2008bcinteraction}. \citeA{zander2008enhancing} subsequently presented a framework formally separating this approach from traditional BCI research, and proposed a name: \emph{passive BCI}. The concept found acceptance also among clinical BCI researchers in 2012 \cite{wolpaw2012newsun}.

A passive BCI \cite{zander2008enhancing,zander2011} system derives its output from automatic, spontaneous brain activity, interpreted in the given context \cite{zander2012context}. This interpreted activity is then used as implicit input to support an ongoing task \cite{rotting2009implicit}. Use of the word ``passive'' here results from a user-centered perspective on HCI. It refers to the role of the end user of a system with respect to the BCI: the underlying signals being automatic, spontaneous brain activity, it is an inherent and defining aspect that the user exerts no effort to actively, explicitly, or voluntarily elicit or modulate this activity. Instead, the user focuses on the task at hand while a passive BCI system, in the background, monitors their brain activity for informative correlates of relevant cognitive or affective states. 

\citeA{zander2011} contrast passive BCI with \emph{active} BCI systems, where the brain activity in question is consciously and purposefully modulated in order to control an application (e.g. motor imagery BCIs; \citeNP{pfurtscheller2001motorimagery}), and with \emph{reactive} BCI systems, which rely on brain activity that is evoked by external stimulation but indirectly modulated through voluntary attention (e.g. P300 amplitudes modulated by attention shifts; \citeNP{farwell1988}).

Care should be taken when presuming that mental states can readily be categorized as ``spontaneous'' versus ``voluntary'', as required by these definitions of BCI systems. Similarly, the activity ultimately used by any BCI system may not precisely fit one such a category. A user who is aware of a passive BCI system might be influenced by the expectations they have of that system, and voluntarily commit attentional resources to make sure that the ``spontaneous'' activity takes place. A user might also attempt to consciously modulate this activity if results are not as expected. The other way around, an active BCI might rely on, or inadvertently make use of, brain activity that is not fully voluntarily controlled by the user. Meditation, as an example, seems to present an ambiguous mixture of both: it is the voluntary attempt to induce a state that is usually only achieved when contextual factors align.

The active/ reactive/ passive distinction used here is a user-oriented one and focuses on conscious intentions. In meditation, the intended purpose of the activity is relaxation itself; not the communication of a state of relaxation for further processing. As a thought experiment, we could ask whether the same user behavior and brain activity would be observed if the user was not aware of their influence over a system. If the answer is ``yes'', then we may say that this behavior and brain activity represent ``natural'' human activity. The implicit input gathered from it is then dissociated from its function as input. This is taken as a core property of the technology discussed in this chapter: the underlying brain activity arises from natural human activity that is not purposefully modulated to cater to the BCI system. Note, however, that by this measure, the categorization of a BCI system as (re)active or passive ultimately depends on the individual user, and not on the system itself.

Indeed, the example of meditation can in fact also be used for an active BCI---when a state of relaxation is induced with the conscious intention of influencing a BCI system---or for reactive BCI, when, in addition, external stimuli are used to cue or achieve that state.
In the last decade, the passive BCI approach (i.e., using natural human brain activity as implicit input) has led to research into a range of different directions. Because of the general applicability of this approach to a wide variety of applications, a truly comprehensive overview is beyond the scope, and not the intention, of this chapter. Instead, we would like to discuss a trend that can be observed in recent work and thought related to passive BCI: a trend toward increased interactivity between the human user and the machine.

``Interactivity'' denotes ``the ability of a computer to respond to a user's input'' \cite{defoxfordinteractivity}. In the following sections, we will describe four categories of systems that all take a neurophysiological signal as their input, but respond to it with varying degrees of complexity. Each presented category represents an increase in interactivity compared to the previous one. Selected examples from past and current research are given to illustrate the different categories. A (partially) hypothetical example accompanies us throughout the chapter, suggesting how a surgeon in the operating theater might be helped by a system from each category. In sequence, the given examples thus illustrate the abovementioned trend towards increased interactivity. The chapter ends with a brief speculation of the future, and a summarizing conclusion.


\section{Mental State Assessment}

\subsection{Introduction}
BCI systems in general, by one definition, consist of three components: input, output, and a translation algorithm that produces the latter based on the former \cite{wolpaw2000firstmeeting}. In this definition, input focuses on the measurement of particular aspects of brain activity, which reflect or correlate to a specific mental state. The detection of such a mental state (inferred from the measured activity) is then translated into a specific computer action (output), for example, the movement of a prosthetic arm or the selection of a letter on a screen. 

Passive BCIs are distinct with respect to the exact cause of the mental state that is measured, requiring it to arise automatically as the result of a natural perception, activity, or thought, without the conscious intention of influencing the BCI system (see the discussion in Section~\ref{pbci:introduction}). Emotions, for example, are not usually induced voluntarily, but experienced as the result of something we perceive, do, or think. Other examples include stress, workload, vigilance, arousal, and surprise.
The measurement of such mental states can be helpful and informative by itself, however, without any computer actions being initiated. Translation, in this case, merely quantifies the measurements but causes no further system changes that are reported back to the human who is being measured. A subset of traditional BCI methodology can thus be used to obtain information concerning a variety of mental states, that is, for \emph{mental state assessment} \cite{muller2008,vanerp2012beyondmedical,zander2011}. The resulting state information can then be analyzed and studied further. The user is assumed to be behaving naturally, as discussed in Section~\ref{pbci:introduction}.

To perform such an analysis, that is to calibrate such a ``state detector'' or \emph{classifier}, individual calibration data are usually gathered in experimental environments where an operator's mental state can be carefully controlled and induced. Signal processing, feature extraction, and classifier calibration are then performed as per the general BCI approach, in order to later be able to assess, from a new recording in a different environment, to what extent the operator's state reflected the experimentally induced one(s). If, for example, a state of engagement can reliably be induced experimentally and neurophysiological correlates of that state can robustly be detected in those recordings, then an index can be generated that quantifies, for other recordings, to what extent these engagement-related features were present. This quantification can be done for complete recordings at once, but also moment by moment, identifying, for example, at what times during the recording the features were most or least pronounced.

In brief, mental state assessment takes a recording of neurophysiological activity, and interprets it by producing a quantitative measure of a given mental state. 

To give a hypothetical example of how this could be used, imagine a surgeon is presented with three different designs for a new teleoperated surgical robot. The goal is to evaluate which of these designs will be most efficient under working conditions. The surgeon performs the same operation using all three designs, while her brain activity is registered. Features of this brain activity are later compared to different recordings of her brain activity, collected during carefully controlled sessions of high and low workload. Based on the gathered data, it can be determined which of the three designs evoked the least workload-correlated brain activity. Furthermore, it may even be seen what phases of the operation saw local increases or decreases in such activity.


\subsection{Examples from the Literature}
\label{pbci:mentalstateassessment:examples}

Mental state monitoring can be particularly useful where more traditional methods have clear disadvantages. In \emph{neuroergonomics}, ``the study of brain and behavior at work'' \cite{parasuraman2003neuroergonomics}, the use of BCI-based methods in order to assess an operator's workload levels may replace traditional ergonomics methods where the operator would need to be interrupted for data collection, for example, to fill out a workload-measuring questionnaire. Workload has been widely recognized as a fundamental issue in ergonomics \cite{wickens2014engineeringpsychology}, but a clash of definitions and constructs and intersubjective differences prevent a uniform and objective measurement \cite{young2015workload}. BCI methodology may here be able to deliver a data-driven approach that circumvents (or complements) conceptual definitions and focuses directly on the neurophysiological correlates of those conditions that are to be measured. In case of workload, a consistent finding reflects a frontal-parietal theta-alpha asymmetry in EEG activity, representing the interaction of the dorsolateral prefrontal cortex and the intraparietal sulcus, which are also described as anterior and posterior attentional systems in controlled attention tasks \cite{gerjets2014workload}.

For example, \citeA{gevins2003workloadhci} used the Multi-Attribute Task Battery \cite{comstock1992matb} to simulate controlled working conditions of three different load levels, and found reliable differences in frontal theta and parietal alpha band power in the continuous EEG recordings. On this basis, they constructed a cognitive workload index that could then be used to analyze later recordings. It performed in line with their hypothesis on other traditional experimental workload tasks as well as during more natural HCI tasks.

Using such precalibrated indices, mental state assessment can be used to, for example, A/B test and compare alternative user interface designs or task conditions with respect to the cognitive states they induce. \citeA{frey2014revieweeghci} review how a number of other constructs (attention/vigilance/fatigue, error recognition, emotions, engagement/flow/immersion) can be used to evaluate user experience during HCI tasks.

Care should be taken that the recorded reference data are ecologically valid and exhibit the neurophysiological features that specifically correlate with the mental state of interest, and not with other aspects of the recording's context. For example, \cite{muhl2014crosscontextworkload} recorded both high and low workload reference data in different affective contexts, and showed that cross-context training improved the classifier's robustness. See also section \ref{pbci:closedloop:examples} for another approach to make the reference data robust to unrelated influences through a careful selection of reference tasks. \citeA{gerjets2014workload} and \citeA{brouwer2015pitfalls} present an extended discussion of pitfalls and lessons learned in mental state assessment research.

Rather than recording reference data indicating extreme ends of the spectrum (e.g. underload versus overload conditions), reference data can also be used as a baseline indicating optimal conditions (e.g., balanced workload). \citeA{zander2012context} used a measure of \emph{deviation from the baseline} as undirected measure of suboptimal performance. Such a detected divergence can then be analyzed more closely to find out the underlying factors. In their paradigm, a goal-oriented, gamified HCI task, the Kullback-Leibler divergence of recorded features increased compared to a baseline measurement in phases where participants felt they had lost control over the interaction. This \emph{loss of control} could perhaps constitute a mixture of workload (compensatory actions) and emotional states (frustration), contributing to the measured neurophysiological differences.

Aside from continuous, frequency-based measurements, features can also be extracted in the time domain of EEG recordings, looking at event-related potentials (ERPs). 

Evoked responses to presented stimuli are said to reflect neuronal processing of those stimuli, and the ERPs' morphology may be modulated by affective and cognitive processes \cite{luck2014erp}. For example, an alternative workload measurement method is to pose an oddball paradigm as a secondary task (e.g., to count the number of relatively rare high-pitch tones among a monotonous sequence of low-pitch tones), but instruct participants to focus attention primarily on another, primary task. The more cognitive load is demanded by the primary task, the less attention can be invested in the secondary task. This decreased attention is then reflected in an amplitude decrease of the P300 component in the ERP following target stimuli in the oddball task \cite{kok2001p3workload}.

\citeA{frey2016eegux} used both a continuous EEG-based measure of workload, calibrated using an experimentally controlled variant of the n-back task \cite{kirchner1958nback}, and a secondary oddball task to measure attention. They found that both measures differ significantly across difficulty levels in a 3D wayfinding game, albeit with different sensitivities. 

ERP components may also reflect differences in higher-level cognitive processing. This assumption is the basis of ERP-based \emph{guilty knowledge tests}, where the goal of the experiment is to find out whether or not the participant possesses any information that they wish to conceal. As such, guilty knowledge tests are a type of lie detector. Participants are shown a series of stimuli, some related to the concealed information (e.g., the used weapon in a crime), others neutral (other weapons, random objects). Should the participant exhibit deviating responses to the related stimuli compared to the neutral ones, then this could be taken as a clue that the participant does possess some information concerning the case in question. Based on differences in their ERPs, \citeA{farwell1991gkt} devised a system that could accurately determine for 87.5\% of participants whether or not they had information concerning mock espionage scenarios that the participants were exposed to as part of the experiment. Ongoing developments in BCI and machine learning algorithms continue to be applied to improve performance in this field \cite<e.g.,>{abootalebi2009gkt}.

Similar to workload, measures of a product or service's quality, attractiveness, or potential value to the user are traditionally gauged using questionnaires or interviews, and there is difficulty with respect to conceptual definitions and objective values. In order to elucidate consumer decisions, marketing researchers have turned to neuroscientific and BCI-based methodology to investigate human consumption from a neurophysiological perspective---\emph{neuromarketing} \cite{lee2007neuromarketing}. For example, \citeA{knutson2007purchases} investigated neural correlates of two often opposing forces in purchase decisions: a product's attractiveness, and its price. In a functional magnetic resonance imaging study, they found discriminative activity in separate cortical areas for these two product attributes. Based on features extracted from the nucleus accumbens and the mesial prefrontal cortex, they were subsequently able to develop an index predicting purchase behavior as exhibited by participants during the experiment.


\subsection{Reflection}
\label{pbci:mentalstateassessment:reflection}

The examples of mental state assessment given here all use an initial recording of brain activity as their basis, and then apply signal processing and classification or other predictive techniques in order to obtain a continuous quantification reflecting mental states, or changes in mental state. By using brain activity, these approaches exhibit a number of relevant differences compared to other methods that attempt to do the same thing (e.g. questionnaires, interviews, introspection, thinking aloud). Brain activity provides a continuous, potentially unobtrusive source of data, and its recording does not interfere with the state to be measured. It enables a functional, data-driven approach to individual, subject-specific state assessment. This can provide a possible alternative to sometimes competing and contradictory theory-driven approaches. It does, however require special care to interpret the resulting data and validate the measures taken: in data-driven approaches, correlations that are not neurophysiologically plausible may be found---for example, co-varying neural responses to confounding variables may show up in cortical areas unrelated to the state of interest, or models may be overfitted on the available data
\cite<e.g.,>{babyak2004overfitting,haufe2014}. Another reason to use neurophysiological measurements may be when the information of interest cannot be obtained otherwise, for example, when a continuous recording and quantification is required, or when the human's own indications may be biased.

Mental state monitoring is an essential element in (passive) BCI systems, but does not constitute such a system in and of itself. As per the abovementioned definition of BCI systems, the third component---output---is missing: the measurement does not trigger any actions, nor does it influence any system, beyond the measurement itself. Rather, the measurement itself is the goal of the use of the technology, as an additional instrument, to be used for later post hoc analysis.

Section~\ref{pbci:openloop} will present examples of passive BCI systems that translate the measurements into system actions in real-time. These actions simultaneously provide feedback to the participants, adding a first element of interactivity to the system. We will also see that this feedback has inherent advantages for the validation of the measured user state. 


\section{Open-Loop Adaptation}
\label{pbci:openloop}

\subsection{Introduction}

HCI describes the interaction between one or more human users and a technical system \cite{preece2015id}. In a typical interaction cycle, an operator (user) gives a command to a machine, which processes the command and gives corresponding feedback to the operator. ``Feedback'' here denotes information about an action returned to the initial causing source of the action---that is, a response from the computer back to the user. This can be either the effect of the action itself or a separate informative signal. 

Even modern-day interaction techniques, for example, touch screens and virtual reality controls, in essence still rely on the same principles as traditional techniques: in order to provide input to the computer, users explicitly activate one element after another, in accordance with the computer's logic (keyboard presses, mouse clicks, virtual buttons, menus, sliders etc.). ``Explicit'' here reflects that the action was voluntarily executed with the conscious and sole intention of providing input to the computer. 

Passive BCI can provide a fundamentally different, \emph{implicit} input channel. ``Implicit'' here is the antonym to explicit, and indicates that the input in question was generated without the source's voluntary intention of doing (merely) that \cite{schmidt2000,rotting2009implicit,zander2016nat}. Passive BCI can be embedded into the human-computer interaction cycle by taking mental state assessments as implicit input, processing this input accordingly, and providing corresponding feedback to the user. This feedback creates interactivity with respect to the BCI: the computer now has the ability to respond to the user's implicit input. 

In the simplest case, one interaction cycle (input---processing---feedback) can be viewed in isolation, following a simple one-time stimulus-response logic. Whenever a specific input is given to the machine, it performs one and the same action. The action serves only to comply with the input as it was given. This is referred to as open-loop adaptation.

Systems from this first category of adaptive applications apply mental state assessment to obtain a measure of a mental state online, and respond to certain states with specific preprogrammed actions in an open-loop fashion.

For our surgeon, the online detection of high phases of workload could for example automatically trigger the system to switch on an indicator light. This communicates to the other members of the surgical team that the surgeon is not to be disturbed with lower-priority interruptions \cite{zander2017surgery}.


\subsection{Examples from the Literature}

One mental state that has received particular attention in the context of general HCI, is the \emph{perception of errors}. Partially due to the artificial and limited nature of traditional interaction techniques \cite{suchman1987hmcproblems,tufte1990}, mistakes are made during ongoing HCI, resulting in frustration, loss of productivity, or, in safety-critical environments, potentially worse \cite{reason1990humanerror}. Upon perceiving the feedback indicating the error, or even already upon executing the erroneous action itself, the human user recognizes that a mistake has been made. Such conscious error perception elicits a much-researched neuroelectric response known as the error-related negativity (ERN; \citeNP{gehring2011ern}). If a system could detect such a negativity in its human user, and link it to a specific action or feedback signal, it could automatically undo the apparently perceived to be erroneous action, or even learn to prevent it in the future.

For example, \citeA{parra2003} report a 21.4\% average reduction (albeit ranging from -6 to +49\%) in errors made by the combined human-machine system on a speeded reaction task, which is the traditional experimental task to elicit response errors. Here, a passive BCI system calibrated to detect an ERN after each human response informed a decision to override the human input. Note that, as the -6\% figure for one participant indicates, this can lead to an increase in errors when the passive BCI itself makes mistakes. 

Testing to what extent these findings would apply to more realistic HCI control scenarios, \citeA{iturrate2012} found similar detectable responses to errors in a series of three experiments that represent essentially a similar task but with different degrees of realism: abstract cursor control, simulated robot control, and actual robot control. Error-related responses have also been found in different HCI contexts and modalities, for example, based on auditory \cite{zander2011music} and tactile \cite{lehne2009tactile} feedback.

Being reliable in various, realistic contexts, such responses can be used to undo or otherwise correct for errors during HCI, as for example, \citeA{zander2010} demonstrated using passive BCI error correction support in a gaming context.

ERN detection has also been used by \citeA{kreilinger2012} to inform the continuous motion of a prosthetic arm. Blinking light-emitting diodes (LEDs) would inform the user whether the arm would continue or stop moving within the upcoming second. When a LED indicating continued movement elicited an error response in the user, this could be interpreted as a passive input command to stop moving, and vice versa. 

The use of another type of cognitive state was proposed by \citeA{protzak2013gazebased} and further investigated \citeA{shishkin2016wishmouse} to support gaze-based HCI. In gaze-based HCI, an eye tracker is used to follow the user's gaze, which can thus be used to control, for example, a pointer on a screen. To select or activate an item, often a \emph{dwell time} is used: if the cursor remains for a certain amount of time on one and the same item (i.e., if the user looks at it for that long), it is activated.  This can lead to a high number of false activations, as not all gazes are intended for interaction. Both \citeA{protzak2013gazebased} and \citeA{shishkin2016wishmouse} compared brain activity following gaze fixations on an item under two conditions: one where interaction was intended (gaze-based control was enabled), and one where it was not (gaze-based control was disabled). They found significant amplitude differences in the ERP following control versus no-control fixations. Shishkin et al. applied a classifier trained on these differences online, in a gaze-controlled selection task. Although their classification system achieved insufficient sensitivity values to fully replace the ``click'' of a mouse, this may one day be possible. A computer that can reliably detect its user's intentions in such a way could greatly reduce the amount of explicit commands required from the user. 

BCI has also been used in gaming environments to effect changes in the ongoing interaction. For example, \citeA{vandelaar2013wow} incorporated BCI into an online role-playing game. Depending on parietal alpha band power, assumed to reflect the player's state of relaxation versus tension, the character they controlled would shape-shift into either an elf (relaxation) or a bear (tension). These character forms had different functional abilities, thus affecting the user's strategy in the game. Note that in the cited case this was a form of \emph{active} BCI, because the experimenters explicitly instructed the participants to control their state of mind in order to consciously shift between shapes. However, since the chosen features (parietal alpha) do naturally correlate to relaxation, this could also be an exemplary implementation of open-loop adaptation based on passive BCI. This highlights the issue discussed in Section~\ref{pbci:introduction}: whether or not a system is active or passive does not depend on the implementation itself, but ultimately on the user's behavior. For similar examples, see \citeA{ilstedt2000brainball,ilstedt2003brainballrd,zander2017phypa}.


\subsection{Reflection}

Open-loop adaptive systems use a measurement of the user's state as implicit input in parallel to an ongoing (inter)action. These can be transient states such as error perception or interaction intent, which should then be carefully tied to the context that evoked them, or more constant states such as moods or other psychophysical conditions, which will then also have a more constant effect on the interaction. As such, we see that the latter is used to implicitly control an application's mode, whereas the former is used to provide more timely implicit executive commands. The use of passive BCI methodology in these interactive systems either supports the user by replacing commands that otherwise needed to be provided explicitly, or provides an extra dimension of experience to the interaction that would not have been possible using traditional input modalities.

An analysis of the effects of the BCI on the interaction can also serve as a validation of the system. For example, a quantification of \emph{errors compensated} or \emph{correct selections} provides a clear measure of the efficacy of the system. As mentioned in section \ref{pbci:mentalstateassessment:reflection}, though, care should still be taken that the implicit input underlying any performance improvement is neurophysiologically plausible. 

The examples mentioned here use passive BCI for implicit human-to-computer communication. The implicit input is processed immediately, corresponding actions are performed, and the feedback given to the users completes the interaction cycle. This, however, is also the full extent of the interactivity at this point: the effects do not reach beyond the current interaction cycle, as there is no closed control loop influencing the next cycle. Both the executive and mode switching actions represent open-loop control, where single, independent state detections result in single, fixed actions from the computer. More interactive applications exhibit closed-loop control, where the given feedback also influences the user, as we shall see next.


\section{Closed-Loop Adaptation}
\label{pbci:closedloop}%


\subsection{Introduction}

In closed-loop systems, output is fed back to the system as new input. In other words, the generated output influences the next cycle. In case of an elementary human-machine system based on passive BCI, the ultimate source of input is the human user's mental state. One method to implement a closed loop is thus to have the produced feedback cause a change in the user state. By influencing the mental state that is measured, the system influences its own next input. 

One motivation for such an implementation may be to promote and sustain a desirable psychological state; for example, an optimally supporting balance of factors, or a state of \emph{flow} \cite{csikszentmihalyi2008flow}. The adaptive loop may, for example, be programmed to promote a state of positive engagement in order to improve performance. This type of system could modify the demands of the task to maximize the user's motivation and enjoyment. When there is a clearly-defined and unambiguously operationalized psychological state that is desirable, then the system can use its adaptive logic and feedback to manipulate the state of the user until the desired target state is reached.

This increases the interactivity of the system in the sense that it is now able to respond purposefully, as well as able to monitor the effect of each response. The given response attempts to qualitatively influence the condition of the ongoing interaction.

Thus, closed-loop adaptive systems apply mental state assessment to obtain a measure of a mental state online, and respond to certain states---or changes in states---with actions that influence that same mental state. 

We can again turn to our earlier example of workload in the operating theater to illustrate this category of applications. When a system detects that the surgeon is currently under high load, it can attempt to directly compensate for this by assuming certain tasks on behalf of the surgeon. By relieving the surgeon from some of her tasks, the system purposefully attempts to lower her load. We will see a similar example from the literature in more detail, described next.


\subsection{Examples from the Literature}
\label{pbci:closedloop:examples}

As mentioned in section \ref{pbci:mentalstateassessment:examples}, a measure of workload is important in ergonomics research in order to inform design decisions. An adaptive system, however, could contain a range of options that would otherwise be decided upon by the designers, and switch between them during online operation depending on current measures of workload. One such option range is the level of automation: a machine can be operated entirely by explicit commands from the operator, or support the human operator to various degrees, up to assuming full autonomy and leaving the user in a mere supervisory role (see \citeNP{parasuraman2000taloa} for an overview and framework describing these options). The purpose of automation is to reduce operator mental workload and alleviate fatigue during sustained performance; however, use of automation is associated with negative consequences such as the out-of-the-loop problems and decay of skilled performance \cite{endsley1995ootluf}. Striking the right balance is important to the well-being and performance of the operator, but the appropriate level of automation varies with fluctuating operator capacities and varying task demands. Therefore, \emph{adaptive automation} \cite{byrne1996adaptiveauto} attempts to match the level of allocation to the current capacity of the operator, measured in real time. 

\citeA{kohlmorgen2007} for example performed EEG measurements during highway driving. Aside from controlling the vehicle, two additional tasks were given to the participants that mimic human-car interaction (an auditory response task) as well contextual distractors during driving (a mental calculation task and a listening comprehension task). All these together defined high-workload phases, which were compared to low-workload phases, where no contextual distractors were present. A classifier calibrated to distinguish these phases was later applied in an online driving condition such that high workload detection resulted in the automatic temporary suspension of the auditory response task. The mean performance on this response task significantly improved because of this intervention. 

Similar adaptive logic can be applied to intelligent tutoring systems designed to deliver educational material in a way that sustains the engagement of the student. For example, the pacing of the learning process can be adjusted dynamically in order to sustain motivation without inducing fatigue, frustration, or information overload. In particular, working memory load is said to need careful management for an optimal learning experience \cite{cowan2014workingmemory}.

\citeA{gerjets2014workload} review theory and practice with respect to online working memory assessment, and present two studies in the context of adaptive learning environments. Special care was taken to select experimentally controllable calibration tasks. A combination of two tasks was used, as these tasks both targeted the same cognitive resources that were to be measured during realistic learning exercises, while at the same time these tasks differed with respect to their demand on executive resources. As such, Gerjets et al. attempted to develop a classifier that could detect working memory load independent of other task properties. Using this classifier, a classification accuracy of 73\% was achieved on single trials of word algebra tasks. Their aim is to apply this classifier in an online adaptive environment, catering to current cognitive abilities of the student.

Such an online application has been presented by \citeA{yuksel2016bach}: they used functional near-infrared spectroscopy (fNIRS) first to differentiate between participants' brain activity playing easy and hard piano pieces, and then to detect their cognitive workload online during the learning of a new piece. The adaptive tutor first presented only one line of musical notes, and added the next line only when workload levels indicated that the previous step had been learned to a sufficient degree. Yuksel et al. report significant increases in performance and learning speed using the fNIRS-based adaptive system as compared to self-paced learning. 

\citeA{ewing2016tetris} applied the concept of closed-loop adaptation to a game, Tetris, adjusting the game's speed in order to maintain a level of optimal engagement. They implemented different adaptive behaviors (i.e. more or less responsive), but found little difference between them. They discuss the issue of finding an appropriate benchmark: taking a manual condition as control, as they did, may provide an inherent difference in user experience that is not related to the exact behavior of the closed-loop adaptive system. A better control may be to compare this adaptive system to alternative methods of adaptation, for example, preprogrammed, random, or ``yoked'' (responding to the user state of another individual). Similar to the earlier distinction between mode switches and executive commands, the examples given above aim to achieve a constant, optimal state in the user, but adaptive systems can also effectuate short-lived closed feedback loops in order to achieve a one-time cognitive state. 

\citeA{kirchner2013brainreading} suggest closed-loop implicit control of an alarm system, making use of BCI methodology to assess whether or not the alarm has been perceived and/or the user intends to act on it. Based on this information, the system could then decide to re-sound the alarm with increased saliency until it has been perceived. A state of nonperception would thus trigger new feedback until a state of perception would be observed. Kirchner et al. report accurate classification rates of perceived versus missed alarms during a demanding robotic teleoperation scenario. In another study, they used these as measures of current load, reversing this logic: nonperception of new instructions indicated that the user was occupied with previous instructions. The new instructions were then delayed, to avoid distractions and promote adequate levels of engagement and performance \cite{kirchner2016multirobotcontrol}.


\subsection{Reflection}

Direct access to a quantification of a certain mental state enables systems to monitor and, with appropriate feedback, influence this state also in a closed loop. This approach can be used to promote and sustain desirable mental states, or indeed to mitigate an undesired state. The goal of this closed-loop control can be to achieve a certain equilibrium with respect to the state of interest, or to push for a given threshold. In the former case, the system's potential influence may need to be bidirectional, allowing for both changes that encourage, and changes that alleviate the target state. 

When considering what is a closed-loop system and what is not, special attention must be given to the exact state that is targeted, which may be related to but different from the state that is measured. For example, error perception, as a transient state in the form of an error-related potential that lasts for less than a second, cannot necessarily be used as such to define a closed-loop system. This would be different, however, if this transient mental state can be interpreted in the given context as, for example, being indicative of a more persistent state of general dissatisfaction.  

A closed feedback loop represents increased interactivity because the feedback in this case does not merely function as an informative response to the user, but in fact \emph{purposefully acts upon} the user. The feedback intends to influence the user, as the user's commands intend to influence the system. Where in our earlier examples we saw implicit human-to-computer communication, now we could speak of an \emph{implicit dialogue} between both agents, cooperating more closely in order to achieve common goals.

This cooperation as exemplified here is still limited by the amount of information available to the system and the simplistic logic of the single loop---for example, the one-dimensional measure of workload can only have a one-dimensional adaptation of automation levels. A next step up would need to tackle the question how systems can achieve a more complex adaptive, cooperative behavior based on the limited information passive BCI methodology can obtain at any single time. One such method will be discussed next.


\section{Automated Adaptation}
\label{pbci:automated}

\subsection{Introduction}

Analysis of ongoing brain activity can give insight into momentary mental states, but by itself, not into their likely causes, or the appropriate responses. In the abovementioned adaptive systems, the context is controlled and specific enough to reasonably assume that, for example, workload is caused by task demands, and increased automation will lead to a reduction in measured load. The interactive functions however are limited to these specific conditions.

Human-human interaction, the benchmark for natural communication and cooperation, relies heavily on a shared understanding of the world, the situation, and the communication partner---a shared model of appropriate behavior and how things are \cite{fischer2001usermodeling}. In this, however, HCI is heavily asymmetrical: the human user can potentially be appraised of all details concerning the state of the computer, but hardly any such information is available vice versa \cite{suchman1987hmcproblems}. For a computer to \emph{understand} its user, it needs information and input that goes beyond the bare necessities for its operation. The applications discussed in this chapter so far provide such information, but use it for preprogrammed one-dimensional response logic---they do not ``understand'' the user, that is, they build no model of higher, more abstract aspects of the user's cognitive or affective behavior. A better-informed system has increased ability to respond appropriately to the user; even to act on the given, known information before any new input is received. This thus constitutes an additional increase in interactivity: now also the system, and not merely the human user, can autonomously decide to act.

In this final category of automated adaptive applications, the systems apply mental state assessment alongside other methods of information gathering and build a model to represent aspects of the user's cognitive or affective responses. It is this automatically generated model, finally, that serves as a basis for the system's own autonomous behavior and adaptations---not (merely) the system's current input.

Our surgeon's load may have been consistently detected to increase beyond sustainable levels after 1~h when using a specific teleoperated robot. Having learned this, a well-informed system could call for an additional assistant ahead of time.


\subsection{Examples from the Literature}

\citeA{zander2014implicit} and \citeA{zander2016nat} used single-trial detections of error-related brain activity not to immediately correct perceived mistakes, but to learn, over time, what strategy the user followed to subjectively make the distinction between errors and non-errors. To that end, the system exhibited different behaviors, and learned which behaviors evoked positive, and which evoked negative responses. These responses were generated in the medial prefrontal cortex, associated with a fundamental source of human intelligence: predictive coding \cite{hawkins2004intelligence}. It thus built a model of the user's higher-level preferences with respect to the presented behaviors. Already during this learning process, the system adapted to exhibit behavior most likely to be perceived positively. 

Specifically, this approach was applied to two-dimensional cursor control. A cursor on a computer screen initially moved randomly. Movements in some directions led to positive responses, while movements in other directions evoked negative responses. Over time, the system learned the pattern behind these responses, and as such, learned where the user wanted the cursor to go. The system could thus steer the cursor toward the intended target. 

Extending their earlier-mentioned experiments, \citeA{iturrate2015teaching} demonstrated a similar principle using a robotic arm. Participants observed its movements while their error responses were tracked in order to inform the arm of appropriate and inappropriate movements.


\subsection{Reflection}

In the two examples given in this section, rather than either having direct consequences or not, mental state measurements were stored along with a description of the contexts (here: computer actions) during which they were observed. As such, a model that correlated user states with different contexts was created. 

These examples were again limited to their respective environments, but the principle can be envisioned to apply in a broader sense. With a reliable measure of, for example, satisfaction versus dissatisfaction (as e.g. in \citeNP{zander2016nat}), as well as an accurate recording of relevant context variables \cite{zander2012context}, a hypothetical ubiquitous passive BCI system could generate such a model for a large number of aspects in a user's daily and working life. By collating contextual factors with measures of the user's mental state, the computer learns to understand this user in much the same way as we humans learn from our own experiences. Subsequently, given a known context, the ubiquitous computer could execute that action that has been observed to be most likely to satisfy the user in that context. 

Although the ultimate goal of this hypothetical system would be the same as of the earlier-mentioned adaptive automation, that is, to promote and sustain a desirable state, the methods by which this can be achieved are not set: the parameters of adaptation are learned automatically by the system itself, on the basis of a continuously updated model of its user.

For a large part, it is the understanding (i.e., the models) that we have of our social and physical context that allows us humans to behave appropriately in various contexts. We have started to build these models as soon as we were born and continue to update them as long as we are capable. For this, we use all information available to us, not just the information that others have explicitly deemed relevant to us. By giving computers access to more information and giving them the freedom to build their own models of their users and the context, a form of artificial intelligence may be created that may finally be seen as an autonomous, interactive agent, just as we see ourselves.

In a far-reaching, hypothetical case of a learning, closed-loop, automatically adapting system, we could imagine a system providing what we want before we can act upon those desires ourselves. With this autonomously operating system originally having learned from our own subjective responses to various actions and contexts, it may at some point become unclear who was the ultimate initiator of the action, by whose authority the action was executed, or who, in fact, ``acted''.  As the lines separating the two interactive agents, human and machine, become blurred, merely the interaction itself remains. 

Given also ethical deliberations with respect to, for example, data ownership, informed consent, the potentially harmful influence of machine over man (the same technique could be used to promote an undesired state, resulting in the computer actively working \emph{against} the user), and outcome responsibility, it is prudent to aim to strike a balance between the two agents' influence on each other. Users should always have access to full information concerning the system's models, processes, goals, and actions. Users should also have ultimate control over, and ownership of, data that are derived from their physiology \cite{fairclough2014confidential}. From a cooperative perspective, the best balance may be one where both agents get to optimally use their strengths while their weaknesses are mitigated by the strengths of the other, while keeping these domains formally separate.


\section{Summary and Conclusion}
In this chapter we have discussed potential applications that build up on the ability to perform real-time analyzes of neurophysiological data, first demonstrated as brain-computer interfacing by Jacques J. Vidal. In 1973, envisioning future progress, Vidal wrote that ``mental decisions and reactions can be probed, in a dimension that both transcends and complements overt behavior'' \cite{vidal1973direct}. This statement appears to accurately capture the motivation of passive BCI research today, in its various forms of applications. In this chapter, these were divided into four distinct categories, based on their level of interactivity.

One category of applications, mental state assessment, uses the quantification of state-specific brain activity as a measurement, for example to replace or support other measurements (e.g. questionnaires, behavioral data). This approach can provide data unobtrusively, can be individually calibrated, and may be able to access information that would otherwise remain hidden. An example from this category is the generation of a workload index based on brain activity, in order to assess the amount of resources that different instruments demand from the surgeon operating them. 

A second category of applications, open-loop adaptation, uses the obtained measurements to inform an open-loop adaptation of the software. Using online state assessment, these systems can respond in real time to the detection of certain states. Continuing the above example, a warning light could switch on in an operating theater when it is detected that the surgeon is currently experiencing high load.

In a further category of applications, closed-loop adaptation, the software adaptation is designed with the explicit purpose to also effect a user adaptation, creating a closed control loop. This provides the system with the ability to not merely detect, but also act upon the detected mental states. To keep the surgeon's workload at a sustainable level, for example, certain systems and operations in the theater may be adaptively automated.

Finally, a category of applications was described where the control logic of the system exceeds pre-determined single-loop control: automated adaptation. The system is given the autonomy to learn and adapt based on a larger amount of information, coupled with a measure of the user's mental state. This also grants the system the ability to act autonomously for, or on behalf of, the user. For the surgeon, an additional assistant may automatically be called as the system predicts workload levels that will exceed otherwise manageable levels.

The first category is neither BCI nor interactive, but serves as a basis for the three following categories. Their interactivity shows in their different abilities to respond to the given input. Since effective passive BCI systems are unobtrusive and inconspicuous, these systems' responses may be similarly hidden. It is for this reason that we refer to \emph{adaptation}, using this as a more generic term that includes traditional feedback (e.g., the movement of a prosthetic limb) but also other ways to process the given input (e.g., updating a user model) or act on the basis of previously-learned information. When adaptation is ultimately based on neurophysiological measures, we refer to these systems collectively as \emph{neuroadaptive} systems \cite{zander2016nat}.

The different adaptive behaviors are not mutually exclusive. An automated adaptation system might, for example, predict what a user intends to do and execute that action in advance, but then, in an open-loop fashion, undo the action when it is detected that the prediction was in error.

The here-presented categorization emphasizes the observable path of passive BCI applications toward increased interactivity.

The investigation of truly interactive systems, in particular based on natural human brain activity, can only be properly researched and developed in natural, real-world interactive settings. The well-controlled experimental conditions will at some point no longer be sufficient to make significant progress. Luckily, easily applicable, comfortable electrodes with sufficient signal quality and stability are being developed and continue to be improved \cite{zander2011dry,mullen2015dry,goverdovsky2016eareeg,zander2017dry}. Increased interest from high-profile commercial ventures is currently providing an effective development infrastructure and budget, at least for specific business interests---but the field as a whole may benefit, since the underlying issues are the same. Finally, when classifiers can be constructed that are independent of the human users and applicable across contexts, their universal usability will provide easy access to mental state assessment, boosting its usage \cite{zander2012diss,wei2015transferlearning,krol2016workload}. When these development reach maturity, real-world implementations of the adaptive systems discussed here may seriously and dramatically alter the many human-system interactions that dominate our everyday lives \cite{mcdowell2013realworld}. \\

This chapter described a trend toward increased interactivity in past and current passive BCI research and development. The history of HCI as a whole, too, can be seen in light of increased interactivity. In early computer systems, input commands were written beforehand and given to the system as complete programs. This either resulted in the successful processing of the program, or, quite bluntly, it did not. It was only later that humans were given real-time control over the system, through, for example, immediate processing and interrupt options \cite{suchman1987hmcproblems}. This provided the first interactive human-computer experience, albeit with ultimate agency solely with the human user. A next step toward interactivity thus appeared to be to give agency also to the computer---allowing it to adapt itself and execute commands that the user did not explicitly ask for. Passive BCI systems, discussed here in their various forms, can provide a source of information to make this computer agency in line with the user's intentions. It can provide this next step in human-computer interactivity, and, as the categorization used here intends to show, may provide the ones after that as well, in the form of advanced neuroadaptive technology.


\section*{Acknowledgments}

Part of this work was supported by the Deutsche Forschungsgemeinschaft (ZA 821/3-1). The authors thank Mahta Mousavi for her comments.


\section*{References}

A shared bibliography starts at page~\pageref{bibliography}.


\clearpage
\pagestyle{plain}
